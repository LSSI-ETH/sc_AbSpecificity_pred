{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928443a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 2. Notebook - Generate ESM variable length embeddings of OVA and RBD antibody sequences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Author: Lena Erlach"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Created: 2024-01-17"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Last modified: 2024-07-17"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "todays_date = str(datetime.now().date())\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"# 2. Notebook - Generate ESM variable length embeddings of OVA and RBD antibody sequences\"\n",
    "    )\n",
    ")\n",
    "display(Markdown(\"Author: Lena Erlach\"))\n",
    "display(Markdown(\"Created: 2024-01-17\"))\n",
    "display(Markdown(f\"Last modified: {todays_date}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e53c9b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Load datasets & preprocess](#Preprocess)\n",
    "- [Generate distance matrix](#Generate-dist-mat)\n",
    "- [Generate embeddings](#Generate-embeddings)\n",
    "    - [1. Embed VH_VL sequences](#EmbedVH_VL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e904e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import abmap\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "\n",
    "\n",
    "# add root directory to path such that the utils_nb file can be imported\n",
    "CONFIG_PATH_OVA = \"../../config_file.txt\"\n",
    "CONFIG_PATH_RBD = \"../../config_file_RBD.txt\"\n",
    "\n",
    "# ROOT dir\n",
    "# setup parser for the config file\n",
    "config_O = configparser.ConfigParser()\n",
    "config_R = configparser.ConfigParser()\n",
    "config_O.read(CONFIG_PATH_OVA)\n",
    "config_R.read(CONFIG_PATH_RBD)\n",
    "ROOT_DIR = config_O[\"ROOT\"][\"ROOT_DIR\"]\n",
    "\n",
    "UTILS_DIR = os.path.join(ROOT_DIR, \"src\")\n",
    "sys.path.append(UTILS_DIR)\n",
    "import utils_nb as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4d17e",
   "metadata": {},
   "source": [
    "### Define variables\n",
    "\n",
    "1. Sequence region to model: \"VDJ_VJ_aaSeq\" Heavy and light chain paired sequences\n",
    "2. Dataset input path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c7fa9",
   "metadata": {},
   "source": [
    "### Load dataset and calculate dist matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11abefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_col = \"VDJ_VJ_aaSeq\"  # column name of the sequence to filter for (VDJ_VJ_aaSeq, VDJ_aaSeq, ...)\n",
    "\n",
    "seq_df_inputPath = {\n",
    "    \"OVA\": os.path.join(ROOT_DIR, config_O[\"PATHS\"][\"SEQ_DF\"]),\n",
    "    \"RBD\": os.path.join(ROOT_DIR, config_R[\"PATHS\"][\"SEQ_DF\"]),\n",
    "}\n",
    "dist_matrix_outputPath_VH_VL = {\n",
    "    \"OVA\": os.path.join(ROOT_DIR, config_O[\"VH_VL_EMBEDPATH\"][\"DISTANCE_MATRIX\"]),\n",
    "    \"RBD\": os.path.join(ROOT_DIR, config_R[\"VH_VL_EMBEDPATH\"][\"DISTANCE_MATRIX\"]),\n",
    "}\n",
    "dist_matrix_outputPath_VH = {\n",
    "    \"OVA\": os.path.join(ROOT_DIR, config_O[\"VH_EMBEDPATH\"][\"DISTANCE_MATRIX\"]),\n",
    "    \"RBD\": os.path.join(ROOT_DIR, config_R[\"VH_EMBEDPATH\"][\"DISTANCE_MATRIX\"]),\n",
    "}\n",
    "\n",
    "cuda_dev_num = 0\n",
    "\n",
    "\n",
    "# embedding paths for VH_VL embeddings\n",
    "out_folder = {\n",
    "    \"OVA\": os.path.join(ROOT_DIR, \"data/processed/embeddings/OVA/\"),\n",
    "    \"RBD\": os.path.join(ROOT_DIR, \"data/processed/embeddings/RBD/\"),\n",
    "}\n",
    "\n",
    "seq_df = {}\n",
    "seqs = {}\n",
    "seqs_VH_VL = {}\n",
    "names = {}\n",
    "\n",
    "for dataset in [\"OVA\", \"RBD\"]:\n",
    "    # load preprocessed dataframe\n",
    "    s_df = pd.read_csv(seq_df_inputPath[dataset])\n",
    "    seq_df[dataset] = s_df[\n",
    "        s_df.seq_complete == True\n",
    "    ]  # filter for complete seqs, just in case\n",
    "    # get indeces/names and sequences 2 lists\n",
    "    names[dataset] = seq_df[dataset].seq_id.tolist()\n",
    "    seqs[dataset] = seq_df[dataset].VDJ_aaSeq.tolist()\n",
    "    seqs_VH_VL[dataset] = seq_df[dataset].VDJ_VJ_aaSeq.tolist()\n",
    "\n",
    "    # # Calculate distance matrix\n",
    "    # for s, path in zip([seqs_VH_VL[dataset], seqs[dataset]], [dist_matrix_outputPath_VH_VL[dataset], dist_matrix_outputPath_VH[dataset]]):\n",
    "    #     distance_matrix = utils.calc_norm_levens_dist(s)\n",
    "    #     np.savetxt(path, distance_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:{}\".format(cuda_dev_num)\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d239cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2 loaded to cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /afs/csail.mit.edu/u/l/lenae011/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the Pre-trained Model!\n",
      "esm2 loaded to cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /afs/csail.mit.edu/u/l/lenae011/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the Pre-trained Model!\n"
     ]
    }
   ],
   "source": [
    "# Using ESM2 (best for functional prediction, e.g. affinity, paratope prediction, etc.)\n",
    "pretrained_path_H = (\n",
    "    \"/data/cb/scratch/lenae/p-GP-LLM-AbPred/models/pretrained/AbMAP_esm2_H.pt\"\n",
    ")\n",
    "pretrained_path_L = (\n",
    "    \"/data/cb/scratch/lenae/p-GP-LLM-AbPred/models/pretrained/AbMAP_esm2_L.pt\"\n",
    ")\n",
    "\n",
    "abmap_H = abmap.load_abmap(\n",
    "    pretrained_path=pretrained_path_H, device=cuda_dev_num, plm_name=\"esm2\"\n",
    ")\n",
    "abmap_L = abmap.load_abmap(\n",
    "    pretrained_path=pretrained_path_L, device=cuda_dev_num, plm_name=\"esm2\"\n",
    ")\n",
    "pretrained_path_H_ls = [pretrained_path_H, pretrained_path_L]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16ea21",
   "metadata": {},
   "source": [
    "This step might take a few minutes to load the model on the GPU..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748e2c9",
   "metadata": {},
   "source": [
    "<a id='EmbedVH_VL'></a> \n",
    "# 1. Embed VH_VL and VH sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659c87e",
   "metadata": {},
   "source": [
    "<a id='EmbedVH_VL-esm'></a>\n",
    "Pass sequences through foundational PLM (ESM-2)\n",
    "\n",
    "This step takes time! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae04f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VH_VL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3622 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:39] Saving 0_s1 H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:40] Saving 0_s1 L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n",
      "  0%|          | 1/3622 [00:02<2:57:46,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:41] Saving 1_s1 H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:43] Saving 1_s1 L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n",
      "  0%|          | 2/3622 [00:06<3:24:22,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:45] Saving 2_s1 H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:47] Saving 2_s1 L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n",
      "  0%|          | 3/3622 [00:10<3:50:41,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:49] Saving 3_s1 H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Non IG chains cannot be numbered with the chothia scheme. These will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17-09:11:51] Saving 3_s1 L\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_typ = \"esm2\"\n",
    "\n",
    "\n",
    "embedding_list, ids_to_drop = {}, {}\n",
    "embedding_list_VH, ids_to_drop_VH = {}, {}\n",
    "\n",
    "for dataset in [\"OVA\", \"RBD\"]:\n",
    "    df = seq_df[dataset]\n",
    "\n",
    "    # Generate the ESM embeddings\n",
    "    ids_to_drop[dataset] = []\n",
    "    emb_ids, embedding_list[dataset], ids_to_drop[dataset] = (\n",
    "        utils.generate_ESM_embedding(\n",
    "            df,\n",
    "            seq_column_HC=\"VDJ_aaSeq\",\n",
    "            seq_column_LC=\"VJ_aaSeq\",\n",
    "            augment=False,\n",
    "            model_typ=\"esm2\",\n",
    "            out_folder=out_folder[dataset],\n",
    "            save_plm=True,\n",
    "            save_PLM_aug=False,\n",
    "            cuda_dev_num=dev,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"ESM VH_VL {dataset} embeddings done!!\")\n",
    "\n",
    "    # Generate the ESM embeddings for VH only\n",
    "    ids_to_drop_VH[dataset] = []\n",
    "    emb_ids, embedding_list_VH[dataset], ids_to_drop_VH[dataset] = (\n",
    "        utils.generate_ESM_embedding(\n",
    "            df,\n",
    "            seq_column_HC=\"VDJ_aaSeq\",\n",
    "            seq_column_LC=\"VJ_aaSeq\",\n",
    "            augment=False,\n",
    "            model_typ=\"esm2\",\n",
    "            VH_only=True,\n",
    "            out_folder=out_folder[dataset],\n",
    "            save_plm=True,\n",
    "            save_PLM_aug=False,\n",
    "            cuda_dev_num=dev,\n",
    "        )\n",
    "    )\n",
    "    print(f\"ESM VH {dataset} embeddings done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load augmented embeddings HC embeddings\n",
    "# emb_inputPath_ESM_aug = os.path.join(ROOT_DIR, config['VH_EMBEDPATH']['ESM2_aug_100_var'])\n",
    "\n",
    "# ### ESM2 augmented - VH_VL\n",
    "# aug_esm_embeddings = utils.load_pickle_embeddings_VH_VL(names, emb_inputPath_ESM_aug, file_suffix= '', embedding_type = 'var')\n",
    "# # aug_esm_embeddings_VH = utils.load_pickle_embeddings(names, emb_inputPath_ESM_aug, file_suffix = '_H')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
