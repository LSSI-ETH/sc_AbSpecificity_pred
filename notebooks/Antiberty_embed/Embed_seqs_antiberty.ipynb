{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Notebook - Antiberty embeddings generation VH_VL seqs - OVA & RBD"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Author: Lena Erlach"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Created: 2024-03-07"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Last modified: 2024-03-21"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime, date\n",
    "\n",
    "todays_date = str(datetime.now().date())\n",
    "\n",
    "display(Markdown(f'# Notebook - Antiberty embeddings generation VH_VL seqs - OVA & RBD'))\n",
    "display(Markdown(f'Author: Lena Erlach'))\n",
    "display(Markdown(f'Created: 2024-03-07'))\n",
    "display(Markdown(f'Last modified: {todays_date}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os, sys, torch, tqdm, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from antiberty import AntiBERTyRunner\n",
    "import configparser\n",
    "\n",
    "sys.path.append('/data/cb/scratch/lenae/p-GP-LLM-AbPred/notebooks/')\n",
    "import AbMAP_analysis.utils_abmap_analysis as utils_a\n",
    "import utils_nb as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Antiberty_Seq_embedding(seq_HL, name, antiberty,\n",
    "                                 out_folder = 'embeddings/', save_plm=True): \n",
    "\n",
    "    '''\n",
    "    Function for generating Anitberty embeddings and saving them to a folder. Generates variable length embeddings of heavy and light chains in out_folder;\n",
    "\n",
    "    params:\n",
    "    seq_HL: list of 2 str, sequences of the heavy and light chains \n",
    "    name: str, seq_id of the sequence\n",
    "    antiberty: loaded AntiBERTyRunner() object\n",
    "    out_folder: str, path to the folder where the embeddings should be saved\n",
    "    '''\n",
    "\n",
    "    ids_to_drop = []\n",
    "\n",
    "    if pd.isna(seq_HL[1]) is True: \n",
    "        VH_only = True \n",
    "    else:\n",
    "        VH_only = False\n",
    "\n",
    "    try:\n",
    "        # embed the sequences\n",
    "        if VH_only is True:\n",
    "            embeddings_r = antiberty.embed(seq_HL[0])\n",
    "            embeddings = [embeddings_r[0][1:-1,:].cpu().numpy(), np.nan]\n",
    "        else:\n",
    "            embeddings_r = antiberty.embed(seq_HL)\n",
    "            embeddings = [embeddings_r[0][1:-1,:].cpu().numpy(), embeddings_r[1][1:-1,:].cpu().numpy()]\n",
    "\n",
    "        # create folder for esm embeddings\n",
    "        if save_plm == True:\n",
    "            out_path_PLM = os.path.join(out_folder)\n",
    "            if not os.path.isdir(out_path_PLM):\n",
    "                os.mkdir(out_path_PLM)\n",
    "\n",
    "            # save the embeddings\n",
    "            for embedding, chain_type in zip([embeddings[0], embeddings[1]], ['H', 'L']):\n",
    "                if chain_type == 'H':\n",
    "                    file_name = '{}_{}.p'.format(name, chain_type)\n",
    "                    #print(os.path.join(out_path_PLM, file_name))\n",
    "\n",
    "                    with open(os.path.join(out_path_PLM, file_name), 'wb') as fh:\n",
    "                        pickle.dump(embedding, fh)\n",
    "\n",
    "                if chain_type == 'L' and VH_only is False:\n",
    "                    file_name = '{}_{}.p'.format(name, chain_type)\n",
    "                    with open(os.path.join(out_path_PLM, file_name), 'wb') as fh:\n",
    "                        pickle.dump(embedding, fh)\n",
    "    \n",
    "\n",
    "    except:\n",
    "        ids_to_drop.append(name)\n",
    "        print('except')\n",
    "    \n",
    "\n",
    "    return embeddings, ids_to_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - OVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parser for the config file\n",
    "CONFIG_PATH = '/data/cb/scratch/lenae/p-GP-LLM-AbPred/notebooks/config_file.txt'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "ROOT_DIR = config['ROOT']['ROOT_DIR']\n",
    "\n",
    "# Set input path Sequences \n",
    "seq_df_inputPath = os.path.join(ROOT_DIR, config['PATHS']['SEQ_DF'])\n",
    "\n",
    "\n",
    "seq_col = \"VDJ_VJ_aaSeq\"    # column name of the sequence to filter for (VDJ_VJ_aaSeq, VDJ_aaSeq, ...)\n",
    "\n",
    "\n",
    "# Set input path CamSol measure\n",
    "camsol_inputPath = os.path.join(ROOT_DIR, 'data/raw/CamSol/CamSol_intrinsic2023-10-06_VDJ_VJ_aaSeq.txt')\n",
    "\n",
    "# embedding paths for VH_VL embeddings\n",
    "out_folder = os.path.join(ROOT_DIR, \"data/processed/embeddings/Antiberty\")\n",
    "\n",
    "\n",
    "##### Setup the GPU support: \n",
    "cuda_dev_num = 4\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:{}\".format(cuda_dev_num)\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed dataframe\n",
    "seq_df = pd.read_csv(seq_df_inputPath)\n",
    "# filter df and drop 129 sequences which was also ignored in ESM embeddings\n",
    "seq_df = seq_df[seq_df.seq_complete == True]\n",
    "seq_df.drop(192, inplace=True)\n",
    "\n",
    "seq_df = seq_df.reset_index(drop=True)\n",
    "\n",
    "# get indeces/names and sequences as lists\n",
    "names = seq_df.seq_id.tolist()\n",
    "seqs_H = seq_df.VDJ_aaSeq.tolist()\n",
    "seqs_L = seq_df.VJ_aaSeq.tolist() \n",
    "seqs_HL = [[seqs_H[i], seqs_L[i]] for i in range(len(seqs_H))]\n",
    "\n",
    "# Load CamSol data\n",
    "camsol_raw = pd.read_csv(camsol_inputPath, sep='\\t')\n",
    "camsol_raw.drop(columns=['intrinsic solubility profile'], inplace=True)\n",
    "\n",
    "# add camsol column to seq_df based on the seq_id\n",
    "camsol_raw.rename(columns={'Name': 'seq_id'}, inplace=True)\n",
    "camsol_raw.drop(192, inplace=True)\n",
    "seq_df = pd.merge(seq_df, camsol_raw, on=['seq_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed sequences with antiberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3807/3807 [01:15<00:00, 50.73it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 512 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# mean over embeddings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [emb_dict[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m---> 17\u001b[0m embeddings_m \u001b[38;5;241m=\u001b[39m \u001b[43mutils_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_over_HL\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m                   \n",
      "File \u001b[0;32m/data/cb/scratch/lenae/p-GP-LLM-AbPred/notebooks/AbMAP_analysis/utils_abmap_analysis.py:260\u001b[0m, in \u001b[0;36mmean_over_HL\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m    256\u001b[0m fl_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# vstack H and L embeddings and take mean over embeddings for a fixed lenght\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     fl_emb_chains \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     fl_embeddings\u001b[38;5;241m.\u001b[39mappend(fl_emb_chains\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(fl_embeddings)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/numpy/core/shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 512 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "ids_to_drop = []\n",
    "# load model\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "# generate embeddings\n",
    "for seq, name in tqdm.tqdm(zip(seqs_HL, names), total=len(seqs_HL)):\n",
    "    # print(name)\n",
    "    embeddings, ids_dropped = generate_Antiberty_Seq_embedding(seq_HL = seq, name= name, antiberty=antiberty,\n",
    "                                    out_folder = out_folder, save_plm=True)\n",
    "    emb_dict[name] = embeddings\n",
    "    ids_to_drop.append(ids_dropped)   \n",
    "\n",
    "\n",
    "# mean over embeddings\n",
    "embeddings = [emb_dict[s] for s in names]\n",
    "embeddings_m = utils_a.mean_over_HL(embeddings)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create function to load embeddings\n",
    "input_folder = out_folder \n",
    "\n",
    "#### Function to load emMbeddings\n",
    "embeddings_loaded = utils.load_pickle_embeddings_VH_VL(names=names, inputPath=input_folder, embedding_type = 'var', file_suffix = '', verbose=False)\n",
    "embeddings_m = utils_a.mean_over_HL(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VH embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_raw = utils.load_pickle_embeddings(names, out_folder, file_suffix = '_H')\n",
    "embeddings_m = np.array([emb.mean(0) for emb in embeddings_raw])\n",
    "embeddings_m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - RBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parser for the config file\n",
    "CONFIG_PATH = '/data/cb/scratch/lenae/p-GP-LLM-AbPred/notebooks/config_file_RBD.txt'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "ROOT_DIR = config['ROOT']['ROOT_DIR']\n",
    "\n",
    "# Set input path Sequences \n",
    "seq_df_inputPath = os.path.join(ROOT_DIR, config['PATHS']['SEQ_DF'])\n",
    "\n",
    "\n",
    "seq_col = \"VDJ_VJ_aaSeq\"    # column name of the sequence to filter for (VDJ_VJ_aaSeq, VDJ_aaSeq, ...)\n",
    "\n",
    "\n",
    "# # Set input path CamSol measure\n",
    "# camsol_inputPath = os.path.join(ROOT_DIR, 'data/raw/CamSol/CamSol_intrinsic2023-10-06_VDJ_VJ_aaSeq.txt')\n",
    "\n",
    "# embedding paths for VH_VL embeddings\n",
    "out_folder = os.path.join(ROOT_DIR, \"data/processed/embeddings/RBD/Antiberty\")\n",
    "\n",
    "\n",
    "##### Setup the GPU support: \n",
    "cuda_dev_num = 4\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:{}\".format(cuda_dev_num)\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed dataframe\n",
    "seq_df = pd.read_csv(seq_df_inputPath)\n",
    "# filter df \n",
    "seq_df = seq_df[seq_df.seq_complete == True]\n",
    "seq_df = seq_df.reset_index(drop=True)\n",
    "\n",
    "# get indeces/names and sequences as lists\n",
    "names = seq_df.seq_id.tolist()\n",
    "seqs_H = seq_df.VDJ_aaSeq.tolist()\n",
    "seqs_L = seq_df.VJ_aaSeq.tolist() \n",
    "seqs_HL = [[seqs_H[i], seqs_L[i]] for i in range(len(seqs_H))]\n",
    "\n",
    "# # Load CamSol data\n",
    "# camsol_raw = pd.read_csv(camsol_inputPath, sep='\\t')\n",
    "# camsol_raw.drop(columns=['intrinsic solubility profile'], inplace=True)\n",
    "\n",
    "# # add camsol column to seq_df based on the seq_id\n",
    "# camsol_raw.rename(columns={'Name': 'seq_id'}, inplace=True)\n",
    "# camsol_raw.drop(192, inplace=True)\n",
    "# seq_df = pd.merge(seq_df, camsol_raw, on=['seq_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [01:43<00:00, 34.77it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "ids_to_drop = []\n",
    "# load model\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "# generate embeddings\n",
    "for seq, name in tqdm.tqdm(zip(seqs_HL, names), total=len(seqs_HL)):\n",
    "    # print(name)\n",
    "    embeddings, ids_dropped = generate_Antiberty_Seq_embedding(seq_HL = seq, name= name, antiberty=antiberty,\n",
    "                                    out_folder = out_folder, save_plm=True)\n",
    "    emb_dict[name] = embeddings\n",
    "    ids_to_drop.append(ids_dropped)   \n",
    "\n",
    "\n",
    "# mean over embeddings\n",
    "embeddings = [emb_dict[s] for s in names]\n",
    "embeddings_m = utils_a.mean_over_HL(embeddings)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
