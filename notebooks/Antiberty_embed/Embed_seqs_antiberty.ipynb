{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Notebook - Antiberty embeddings generation VH_VL seqs - OVA & RBD"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Author: Lena Erlach"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Created: 2024-03-07"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Last modified: 2024-03-21"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "todays_date = str(datetime.now().date())\n",
    "\n",
    "display(Markdown(\"# Notebook - Antiberty embeddings generation VH_VL seqs - OVA & RBD\"))\n",
    "display(Markdown(\"Author: Lena Erlach\"))\n",
    "display(Markdown(\"Created: 2024-03-07\"))\n",
    "display(Markdown(f\"Last modified: {todays_date}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/data/cb/scratch/lenae/software/Anaconda/envs/abmap/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from antiberty import AntiBERTyRunner\n",
    "import configparser\n",
    "\n",
    "sys.path.append(\"../../src/\")\n",
    "import utils_nb as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Antiberty_Seq_embedding(\n",
    "    seq_HL, name, antiberty, out_folder=\"embeddings/\", save_plm=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for generating Anitberty embeddings and saving them to a folder. Generates variable length embeddings of heavy and light chains in out_folder;\n",
    "\n",
    "    params:\n",
    "    seq_HL: list of 2 str, sequences of the heavy and light chains\n",
    "    name: str, seq_id of the sequence\n",
    "    antiberty: loaded AntiBERTyRunner() object\n",
    "    out_folder: str, path to the folder where the embeddings should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    ids_to_drop = []\n",
    "\n",
    "    if pd.isna(seq_HL[1]):\n",
    "        VH_only = True\n",
    "    else:\n",
    "        VH_only = False\n",
    "\n",
    "    try:\n",
    "        # embed the sequences\n",
    "        if VH_only:\n",
    "            embeddings_r = antiberty.embed(seq_HL[0])\n",
    "            embeddings = [embeddings_r[0][1:-1, :].cpu().numpy(), np.nan]\n",
    "        else:\n",
    "            embeddings_r = antiberty.embed(seq_HL)\n",
    "            embeddings = [\n",
    "                embeddings_r[0][1:-1, :].cpu().numpy(),\n",
    "                embeddings_r[1][1:-1, :].cpu().numpy(),\n",
    "            ]\n",
    "\n",
    "        # create folder for esm embeddings\n",
    "        if save_plm:\n",
    "            out_path_PLM = os.path.join(out_folder)\n",
    "            if not os.path.isdir(out_path_PLM):\n",
    "                os.mkdir(out_path_PLM)\n",
    "\n",
    "            # save the embeddings\n",
    "            for embedding, chain_type in zip(\n",
    "                [embeddings[0], embeddings[1]], [\"H\", \"L\"]\n",
    "            ):\n",
    "                if chain_type == \"H\":\n",
    "                    file_name = \"{}_{}.p\".format(name, chain_type)\n",
    "                    # print(os.path.join(out_path_PLM, file_name))\n",
    "\n",
    "                    with open(os.path.join(out_path_PLM, file_name), \"wb\") as fh:\n",
    "                        pickle.dump(embedding, fh)\n",
    "\n",
    "                if chain_type == \"L\" and not VH_only:\n",
    "                    file_name = \"{}_{}.p\".format(name, chain_type)\n",
    "                    with open(os.path.join(out_path_PLM, file_name), \"wb\") as fh:\n",
    "                        pickle.dump(embedding, fh)\n",
    "\n",
    "    except:\n",
    "        ids_to_drop.append(name)\n",
    "        print(\"except\")\n",
    "\n",
    "    return embeddings, ids_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - OVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parser for the config file ../../example_config_file.txt\n",
    "CONFIG_PATH = \"../../config_file.txt\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "ROOT_DIR = config[\"ROOT\"][\"ROOT_DIR\"]\n",
    "\n",
    "# Set input path Sequences\n",
    "seq_df_inputPath = os.path.join(ROOT_DIR, config[\"PATHS\"][\"SEQ_DF\"])\n",
    "\n",
    "\n",
    "seq_col = \"VDJ_VJ_aaSeq\"  # column name of the sequence to filter for (VDJ_VJ_aaSeq, VDJ_aaSeq, ...)\n",
    "\n",
    "\n",
    "# Set input path CamSol measure\n",
    "camsol_inputPath = os.path.join(\n",
    "    ROOT_DIR, \"data/raw/CamSol/CamSol_intrinsic2023-10-06_VDJ_VJ_aaSeq.txt\"\n",
    ")\n",
    "\n",
    "# embedding paths for VH_VL embeddings\n",
    "out_folder = os.path.join(ROOT_DIR, \"data/processed/embeddings/Antiberty\")\n",
    "\n",
    "\n",
    "##### Setup the GPU support:\n",
    "cuda_dev_num = 4\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:{}\".format(cuda_dev_num)\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed dataframe\n",
    "seq_df = pd.read_csv(seq_df_inputPath)\n",
    "# filter df and drop 129 sequences which was also ignored in ESM embeddings\n",
    "seq_df = seq_df[seq_df.seq_complete == True]\n",
    "seq_df.drop(192, inplace=True)\n",
    "\n",
    "seq_df = seq_df.reset_index(drop=True)\n",
    "\n",
    "# get indeces/names and sequences as lists\n",
    "names = seq_df.seq_id.tolist()\n",
    "seqs_H = seq_df.VDJ_aaSeq.tolist()\n",
    "seqs_L = seq_df.VJ_aaSeq.tolist()\n",
    "seqs_HL = [[seqs_H[i], seqs_L[i]] for i in range(len(seqs_H))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed sequences with antiberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3621/3621 [01:04<00:00, 56.44it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "ids_to_drop = []\n",
    "# load model\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "# generate embeddings\n",
    "for seq, name in tqdm.tqdm(zip(seqs_HL, names), total=len(seqs_HL)):\n",
    "    # print(name)\n",
    "    embeddings, ids_dropped = generate_Antiberty_Seq_embedding(\n",
    "        seq_HL=seq, name=name, antiberty=antiberty, out_folder=out_folder, save_plm=True\n",
    "    )\n",
    "    emb_dict[name] = embeddings\n",
    "    ids_to_drop.append(ids_dropped)\n",
    "\n",
    "\n",
    "# mean over embeddings\n",
    "embeddings = [emb_dict[s] for s in names]\n",
    "embeddings_m = utils.mean_over_HL(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create function to load embeddings\n",
    "input_folder = out_folder\n",
    "\n",
    "#### Function to load emMbeddings\n",
    "embeddings_loaded = utils.load_pickle_embeddings_VH_VL(\n",
    "    names=names,\n",
    "    inputPath=input_folder,\n",
    "    embedding_type=\"var\",\n",
    "    file_suffix=\"\",\n",
    "    verbose=False,\n",
    ")\n",
    "embeddings_m = utils.mean_over_HL(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VH embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_raw = utils.load_pickle_embeddings(names, out_folder, file_suffix=\"_H\")\n",
    "embeddings_m = np.array([emb.mean(0) for emb in embeddings_raw])\n",
    "embeddings_m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - RBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parser for the config file\n",
    "CONFIG_PATH = \"../../config_file_RBD.txt\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "ROOT_DIR = config[\"ROOT\"][\"ROOT_DIR\"]\n",
    "\n",
    "# Set input path Sequences\n",
    "seq_df_inputPath = os.path.join(ROOT_DIR, config[\"PATHS\"][\"SEQ_DF\"])\n",
    "\n",
    "\n",
    "seq_col = \"VDJ_VJ_aaSeq\"  # column name of the sequence to filter for (VDJ_VJ_aaSeq, VDJ_aaSeq, ...)\n",
    "\n",
    "\n",
    "# embedding paths for VH_VL embeddings\n",
    "out_folder = os.path.join(ROOT_DIR, \"data/processed/embeddings/RBD/Antiberty\")\n",
    "\n",
    "\n",
    "##### Setup the GPU support:\n",
    "cuda_dev_num = 4\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:{}\".format(cuda_dev_num)\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed dataframe\n",
    "seq_df = pd.read_csv(seq_df_inputPath)\n",
    "# filter df\n",
    "seq_df = seq_df[seq_df.seq_complete == True]\n",
    "seq_df = seq_df.reset_index(drop=True)\n",
    "\n",
    "# get indeces/names and sequences as lists\n",
    "names = seq_df.seq_id.tolist()\n",
    "seqs_H = seq_df.VDJ_aaSeq.tolist()\n",
    "seqs_L = seq_df.VJ_aaSeq.tolist()\n",
    "seqs_HL = [[seqs_H[i], seqs_L[i]] for i in range(len(seqs_H))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3593/3593 [01:43<00:00, 34.77it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "ids_to_drop = []\n",
    "# load model\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "# generate embeddings\n",
    "for seq, name in tqdm.tqdm(zip(seqs_HL, names), total=len(seqs_HL)):\n",
    "    # print(name)\n",
    "    embeddings, ids_dropped = generate_Antiberty_Seq_embedding(\n",
    "        seq_HL=seq, name=name, antiberty=antiberty, out_folder=out_folder, save_plm=True\n",
    "    )\n",
    "    emb_dict[name] = embeddings\n",
    "    ids_to_drop.append(ids_dropped)\n",
    "\n",
    "\n",
    "# mean over embeddings\n",
    "embeddings = [emb_dict[s] for s in names]\n",
    "embeddings_m = utils.mean_over_HL(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
